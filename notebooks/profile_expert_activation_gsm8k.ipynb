{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Profiling Expert Activation in GPT-OSS (Base vs BlockFFN-LoRA)\n",
        "\n",
        "This notebook profiles Mixture-of-Experts (MoE) router activations in:\n",
        "- Base: `openai/gpt-oss-20b`\n",
        "- Fine-tuned: `mcemri/gpt-oss-20b-blockffn-lora`\n",
        "\n",
        "We run on a downstream task (GSM8K) and record expert activation patterns per layer, computing useful metrics and plots:\n",
        "- Per-layer activation entropy\n",
        "- Token-level sparsity (TLS) at threshold\n",
        "- Locality metric across adjacent tokens (BlockFFN-style)\n",
        "- Chunk union sparsity (BlockFFN-style)\n",
        "- Top-1 expert usage heatmaps (layers × experts)\n",
        "\n",
        "Reference model on the Hub: [mcemri/gpt-oss-20b-blockffn-lora](https://huggingface.co/mcemri/gpt-oss-20b-blockffn-lora)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/mert_cemri/miniconda3/envs/moeft/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Lazy installs if missing\n",
        "import importlib, subprocess, sys\n",
        "\n",
        "def ensure(pkg: str, pip_name: Optional[str] = None):\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name or pkg])\n",
        "\n",
        "for pkg, pipn in [(\"datasets\", None), (\"transformers\", None), (\"accelerate\", None), (\"seaborn\", None)]:\n",
        "    ensure(pkg, pipn)\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DTYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "# Plotting defaults\n",
        "sns.set_context(\"talk\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
        "plt.rcParams[\"figure.dpi\"] = 120\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f09900cb930>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuration\n",
        "BASE_MODEL_ID = \"openai/gpt-oss-20b\"\n",
        "FT_MODEL_ID = \"mcemri/gpt-oss-20b-blockffn-lora\"  # merged model on Hub\n",
        "\n",
        "DATASET_NAME = \"gsm8k\"\n",
        "DATASET_CONFIG = \"main\"\n",
        "SPLIT = \"test\"  # or \"train\"\n",
        "NUM_SAMPLES = 50  # adjust for runtime; large models are heavy\n",
        "SEED = 42\n",
        "\n",
        "# Generation / profiling\n",
        "MAX_NEW_TOKENS = 0  # set >0 to also profile decode steps (slower)\n",
        "TEMPERATURE = 0.0\n",
        "TOP_P = 1.0\n",
        "\n",
        "# Router capture\n",
        "ROUTER_KEYS = [\n",
        "    \"router_logits\",\n",
        "    \"router_probs\",\n",
        "    \"router_probabilities\",\n",
        "    \"gate_logits\",\n",
        "    \"gating_logits\",\n",
        "]\n",
        "CHUNK_LEN = 8\n",
        "PROB_TEMPERATURE = 1.0\n",
        "MIN_PROB_EPS = 1e-6\n",
        "ENTROPY_EPS = 1e-8\n",
        "TLS_THRESHOLD = 0.01\n",
        "SIGMOID_ALPHA = 12.0\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _flatten_tensors(value) -> List[torch.Tensor]:\n",
        "    tensors: List[torch.Tensor] = []\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        tensors.append(value)\n",
        "    elif isinstance(value, (list, tuple)):\n",
        "        for item in value:\n",
        "            tensors.extend(_flatten_tensors(item))\n",
        "    elif isinstance(value, dict):\n",
        "        for item in value.values():\n",
        "            tensors.extend(_flatten_tensors(item))\n",
        "    return tensors\n",
        "\n",
        "\n",
        "def softmax_router(t: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "    x = t.to(torch.float32)\n",
        "    # Accept [T, E] by assuming batch=1\n",
        "    if x.dim() == 2:\n",
        "        x = x.unsqueeze(0)\n",
        "    elif x.dim() < 2:\n",
        "        return torch.empty(0, device=x.device)\n",
        "    if temperature != 1.0:\n",
        "        x = x / temperature\n",
        "    probs = torch.softmax(x, dim=-1)\n",
        "    return torch.clamp(probs, min=MIN_PROB_EPS)\n",
        "\n",
        "\n",
        "def collect_from_output(outputs, router_keys=ROUTER_KEYS) -> List[torch.Tensor]:\n",
        "    if isinstance(outputs, dict):\n",
        "        lookup = outputs\n",
        "    else:\n",
        "        lookup = {k: getattr(outputs, k) for k in dir(outputs) if not k.startswith(\"_\")}\n",
        "    tensors: List[torch.Tensor] = []\n",
        "    for key in router_keys:\n",
        "        if key in lookup:\n",
        "            tensors.extend(_flatten_tensors(lookup[key]))\n",
        "    return tensors\n",
        "\n",
        "\n",
        "_LAYER_RE = re.compile(r\"layers\\.(\\d+)\\.\")\n",
        "\n",
        "\n",
        "def _extract_layer_idx(name: str) -> Optional[int]:\n",
        "    m = _LAYER_RE.search(name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def register_router_hooks(model) -> Tuple[Dict[int, List[torch.Tensor]], List[torch.utils.hooks.RemovableHandle]]:\n",
        "    captured: Dict[int, List[torch.Tensor]] = {}\n",
        "    handles: List[torch.utils.hooks.RemovableHandle] = []\n",
        "\n",
        "    def make_hook(layer_idx: int):\n",
        "        def _hook(module, inputs, output):\n",
        "            # We accept either logits or probs; convert to probs later\n",
        "            out_tensors = []\n",
        "            if isinstance(output, torch.Tensor):\n",
        "                out_tensors.append(output.detach())\n",
        "            else:\n",
        "                out_tensors.extend(_flatten_tensors(output))\n",
        "            if out_tensors:\n",
        "                captured.setdefault(layer_idx, []).extend([t.detach() for t in out_tensors])\n",
        "        return _hook\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        lname = name.lower()\n",
        "        should_hook = False\n",
        "        if lname.endswith(\"mlp.router\") or lname.endswith(\".router\"):\n",
        "            should_hook = True\n",
        "        elif (\"router\" in lname or \"gating\" in lname or \"gate\" in lname) and (\"mlp\" in lname or \"expert\" in lname):\n",
        "            should_hook = True\n",
        "        if should_hook:\n",
        "            layer_idx = _extract_layer_idx(name)\n",
        "            if layer_idx is None:\n",
        "                continue\n",
        "            handles.append(module.register_forward_hook(make_hook(layer_idx)))\n",
        "\n",
        "    return captured, handles\n",
        "\n",
        "\n",
        "def consolidate_captured(captured: Dict[int, List[torch.Tensor]], to_probs: bool = True) -> Dict[int, List[torch.Tensor]]:\n",
        "    consolidated: Dict[int, List[torch.Tensor]] = {}\n",
        "    for layer_idx, chunks in captured.items():\n",
        "        if not chunks:\n",
        "            continue\n",
        "        cat = []\n",
        "        for t in chunks:\n",
        "            if t.numel() == 0:\n",
        "                continue\n",
        "            # Normalize shape to [B, T, E] when needed before converting\n",
        "            x = t\n",
        "            if x.dim() == 2:\n",
        "                x = x.unsqueeze(0)\n",
        "            if to_probs:\n",
        "                cat.append(softmax_router(x, temperature=PROB_TEMPERATURE))\n",
        "            else:\n",
        "                cat.append(x)\n",
        "        if cat:\n",
        "            consolidated[layer_idx] = cat\n",
        "    return consolidated\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy_from_probs(p: torch.Tensor, eps: float = ENTROPY_EPS) -> torch.Tensor:\n",
        "    p = torch.clamp(p, min=eps)\n",
        "    return -(p * torch.log(p)).sum(dim=-1)\n",
        "\n",
        "\n",
        "def estimate_token_sparsity(probs: torch.Tensor, mask: torch.Tensor, threshold: float = TLS_THRESHOLD) -> Optional[torch.Tensor]:\n",
        "    masked = mask.unsqueeze(-1).float()\n",
        "    denom = masked.sum()\n",
        "    if denom == 0:\n",
        "        return None\n",
        "    active = (probs > threshold).float()\n",
        "    ratio = (active * masked).sum() / (denom * probs.size(-1))\n",
        "    return 1.0 - ratio\n",
        "\n",
        "\n",
        "def compute_activation_locality_metric(\n",
        "    probs_list: List[torch.Tensor], token_mask: torch.Tensor, alpha: float = SIGMOID_ALPHA\n",
        ") -> Tuple[Optional[torch.Tensor], dict]:\n",
        "    if not probs_list:\n",
        "        return None, {}\n",
        "    losses, tls_vals = [], []\n",
        "    for probs in probs_list:\n",
        "        if probs.size(1) < 2:\n",
        "            continue\n",
        "        mask_forward = token_mask[:, :-1] & token_mask[:, 1:]\n",
        "        if not mask_forward.any():\n",
        "            continue\n",
        "        sharpen_curr = torch.sigmoid(alpha * (probs[:, :-1, :] - 0.5))\n",
        "        sharpen_next = torch.sigmoid(alpha * (probs[:, 1:, :] - 0.5))\n",
        "        bce = torch.nn.functional.binary_cross_entropy(sharpen_next, sharpen_curr, reduction=\"none\").mean(dim=-1)\n",
        "        loss = (bce * mask_forward.float()).sum() / mask_forward.float().sum()\n",
        "        losses.append(loss.detach())\n",
        "        tls = estimate_token_sparsity(probs, token_mask)\n",
        "        if tls is not None:\n",
        "            tls_vals.append(tls.detach())\n",
        "    if not losses:\n",
        "        return None, {}\n",
        "    metrics = {}\n",
        "    if tls_vals:\n",
        "        metrics[\"tls\"] = torch.stack(tls_vals).mean().item()\n",
        "    return torch.stack(losses).mean(), metrics\n",
        "\n",
        "\n",
        "def compute_chunk_union_sparsity(\n",
        "    probs_list: List[torch.Tensor], token_mask: torch.Tensor, chunk_len: int = CHUNK_LEN, eps: float = MIN_PROB_EPS\n",
        ") -> Tuple[Optional[torch.Tensor], dict]:\n",
        "    if chunk_len <= 0 or not probs_list:\n",
        "        return None, {}\n",
        "    losses, cls_vals = [], []\n",
        "    for probs in probs_list:\n",
        "        if probs.size(1) < chunk_len:\n",
        "            continue\n",
        "        unfolded_probs = probs.unfold(dimension=1, size=chunk_len, step=1)\n",
        "        unfolded_mask = token_mask.float().unfold(dimension=1, size=chunk_len, step=1)\n",
        "        valid_mask = (unfolded_mask.sum(-1) == float(chunk_len))\n",
        "        if not valid_mask.any():\n",
        "            continue\n",
        "        complement = torch.clamp(1.0 - unfolded_probs, min=eps, max=1.0)\n",
        "        union = 1.0 - torch.prod(complement, dim=-2)\n",
        "        union_mean = union.mean(dim=-1)\n",
        "        loss = (union_mean * valid_mask.float()).sum() / valid_mask.float().sum()\n",
        "        losses.append(loss.detach())\n",
        "        denom = valid_mask.float().sum()\n",
        "        if denom > 0:\n",
        "            sparsity = 1.0 - (union * valid_mask.unsqueeze(-1).float()).sum() / (denom * union.size(-1))\n",
        "            cls_vals.append(sparsity.detach())\n",
        "    if not losses:\n",
        "        return None, {}\n",
        "    metrics = {}\n",
        "    if cls_vals:\n",
        "        metrics[\"cls\"] = torch.stack(cls_vals).mean().item()\n",
        "    return torch.stack(losses).mean(), metrics\n",
        "\n",
        "\n",
        "def top1_counts(probs_list: List[torch.Tensor], token_mask: torch.Tensor) -> Optional[torch.Tensor]:\n",
        "    if not probs_list:\n",
        "        return None\n",
        "    counts = None\n",
        "    for probs in probs_list:\n",
        "        if probs.numel() == 0:\n",
        "            continue\n",
        "        top1 = probs.argmax(dim=-1)  # [B, T]\n",
        "        mask = token_mask\n",
        "        if counts is None:\n",
        "            counts = torch.zeros(probs.size(-1), dtype=torch.long)\n",
        "        counts.index_add_(0, top1[mask].view(-1).cpu(), torch.ones_like(top1[mask].view(-1), dtype=torch.long))\n",
        "    return counts\n",
        "\n",
        "\n",
        "def metrics_from_probs(probs_chunks: List[torch.Tensor], token_mask: torch.Tensor) -> Dict[str, float]:\n",
        "    metrics: Dict[str, float] = {}\n",
        "    if not probs_chunks:\n",
        "        return metrics\n",
        "    # Entropy\n",
        "    ent_vals = []\n",
        "    for probs in probs_chunks:\n",
        "        ent = entropy_from_probs(probs).mean()  # average over tokens\n",
        "        ent_vals.append(ent.detach().item())\n",
        "    metrics[\"entropy_mean\"] = float(np.mean(ent_vals)) if ent_vals else float(\"nan\")\n",
        "    # Locality and TLS\n",
        "    loc_loss, loc_metrics = compute_activation_locality_metric(probs_chunks, token_mask)\n",
        "    if loc_loss is not None:\n",
        "        metrics[\"locality_loss\"] = loc_loss.item()\n",
        "    metrics.update({f\"{k}\": v for k, v in loc_metrics.items()})\n",
        "    # Chunk union sparsity\n",
        "    cls_loss, cls_metrics = compute_chunk_union_sparsity(probs_chunks, token_mask)\n",
        "    if cls_loss is not None:\n",
        "        metrics[\"chunk_union\"] = cls_loss.item()\n",
        "    metrics.update({f\"{k}\": v for k, v in cls_metrics.items()})\n",
        "    return metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(model_id: str):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=DTYPE,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    # Try to enable router outputs if supported by the model\n",
        "    if hasattr(model, \"config\") and hasattr(model.config, \"output_router_logits\"):\n",
        "        model.config.output_router_logits = True\n",
        "    return model, tok\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt(question: str) -> str:\n",
        "    return (\n",
        "        \"You are a helpful math assistant. Solve the following problem and give the final answer as a number.\\n\\n\"\n",
        "        f\"Problem: {question}\\n\\nLet's think step by step.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def load_gsm8k(num_samples: int = NUM_SAMPLES, split: str = SPLIT) -> List[dict]:\n",
        "    ds = load_dataset(DATASET_NAME, DATASET_CONFIG, split=split)\n",
        "    ds = ds.shuffle(seed=SEED).select(range(min(num_samples, len(ds))))\n",
        "    samples = []\n",
        "    for ex in ds:\n",
        "        samples.append({\n",
        "            \"question\": ex[\"question\"],\n",
        "            \"answer\": ex.get(\"answer\", \"\"),\n",
        "            \"prompt\": build_prompt(ex[\"question\"]) ,\n",
        "        })\n",
        "    return samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_single_forward(model, tok, prompt: str):\n",
        "    enc = tok(prompt, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
        "    token_mask = enc.get(\"attention_mask\", (enc[\"input_ids\"] != tok.pad_token_id).long()).bool()\n",
        "\n",
        "    captured, handles = register_router_hooks(model)\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            outputs = model(**enc, use_cache=False, output_router_logits=True)\n",
        "        except TypeError:\n",
        "            outputs = model(**enc, use_cache=False)\n",
        "    for h in handles:\n",
        "        try:\n",
        "            h.remove()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Fallback: collect router tensors directly from model outputs if any\n",
        "    if not captured:\n",
        "        extra = collect_from_output(outputs, ROUTER_KEYS)\n",
        "        if extra:\n",
        "            captured[-1] = [t.detach() for t in extra if isinstance(t, torch.Tensor) and t.numel() > 0]\n",
        "\n",
        "    probs_by_layer = consolidate_captured(captured, to_probs=True)\n",
        "    # Ensure shapes are [B, T, E] and on CPU for metrics\n",
        "    for k, chunks in list(probs_by_layer.items()):\n",
        "        fixed = []\n",
        "        for t in chunks:\n",
        "            if t.numel() == 0:\n",
        "                continue\n",
        "            if t.dim() == 2:  # [T, E] -> [1, T, E]\n",
        "                t = t.unsqueeze(0)\n",
        "            fixed.append(t.cpu())\n",
        "        probs_by_layer[k] = fixed\n",
        "\n",
        "    token_mask_cpu = token_mask.cpu()\n",
        "\n",
        "    metrics_layer: Dict[int, Dict[str, float]] = {}\n",
        "    counts_layer: Dict[int, torch.Tensor] = {}\n",
        "\n",
        "    for layer_idx, probs_chunks in probs_by_layer.items():\n",
        "        m = metrics_from_probs(probs_chunks, token_mask_cpu)\n",
        "        metrics_layer[layer_idx] = m\n",
        "        c = top1_counts(probs_chunks, token_mask_cpu)\n",
        "        if c is not None:\n",
        "            counts_layer[layer_idx] = c\n",
        "\n",
        "    return metrics_layer, counts_layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def profile_model(samples: List[dict], model_id: str):\n",
        "    model, tok = load_model_and_tokenizer(model_id)\n",
        "    try:\n",
        "        per_layer_metrics: Dict[int, List[Dict[str, float]]] = {}\n",
        "        per_layer_counts: Dict[int, torch.Tensor] = {}\n",
        "\n",
        "        for ex in tqdm(samples, desc=f\"Profiling {model_id}\"):\n",
        "            m, c = run_single_forward(model, tok, ex[\"prompt\"])\n",
        "            for layer_idx, md in m.items():\n",
        "                per_layer_metrics.setdefault(layer_idx, []).append(md)\n",
        "            for layer_idx, counts in c.items():\n",
        "                if layer_idx not in per_layer_counts:\n",
        "                    per_layer_counts[layer_idx] = counts.clone()\n",
        "                else:\n",
        "                    per_layer_counts[layer_idx] += counts\n",
        "\n",
        "        # Aggregate metrics by mean across samples\n",
        "        agg_metrics: Dict[int, Dict[str, float]] = {}\n",
        "        all_keys = set()\n",
        "        for arr in per_layer_metrics.values():\n",
        "            if arr:\n",
        "                all_keys.update(arr[0].keys())\n",
        "        for layer_idx, arr in per_layer_metrics.items():\n",
        "            if not arr:\n",
        "                continue\n",
        "            d = {}\n",
        "            for k in all_keys:\n",
        "                vals = [x.get(k) for x in arr if k in x and not (x.get(k) is None or np.isnan(x.get(k)))]\n",
        "                d[k] = float(np.mean(vals)) if vals else float(\"nan\")\n",
        "            agg_metrics[layer_idx] = d\n",
        "\n",
        "        # Normalize counts to fractions per layer\n",
        "        frac_counts: Dict[int, np.ndarray] = {}\n",
        "        for layer_idx, counts in per_layer_counts.items():\n",
        "            total = counts.sum().item()\n",
        "            if total > 0:\n",
        "                frac_counts[layer_idx] = (counts.float() / total).numpy()\n",
        "        return agg_metrics, frac_counts\n",
        "    finally:\n",
        "        try:\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plots_compare_metrics(base_metrics: Dict[int, Dict[str, float]], ft_metrics: Dict[int, Dict[str, float]]):\n",
        "    layers = sorted(set(base_metrics.keys()) | set(ft_metrics.keys()))\n",
        "    metric_names = set()\n",
        "    for d in base_metrics.values():\n",
        "        metric_names.update(d.keys())\n",
        "    for d in ft_metrics.values():\n",
        "        metric_names.update(d.keys())\n",
        "    metric_names = [m for m in [\"entropy_mean\", \"tls\", \"locality_loss\", \"chunk_union\", \"cls\"] if m in metric_names]\n",
        "\n",
        "    n = len(metric_names)\n",
        "    if n == 0:\n",
        "        print(\"No metrics to plot.\")\n",
        "        return None\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(12, 4*n), sharex=True)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, m in zip(axes, metric_names):\n",
        "        yb = [base_metrics.get(l, {}).get(m, np.nan) for l in layers]\n",
        "        yf = [ft_metrics.get(l, {}).get(m, np.nan) for l in layers]\n",
        "        ax.plot(layers, yb, label=f\"Base {m}\", marker=\"o\")\n",
        "        ax.plot(layers, yf, label=f\"FT {m}\", marker=\"o\")\n",
        "        ax.set_ylabel(m)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend()\n",
        "    axes[-1].set_xlabel(\"Layer\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plots_compare_heatmaps(base_frac: Dict[int, np.ndarray], ft_frac: Dict[int, np.ndarray]):\n",
        "    layers = sorted(set(base_frac.keys()) | set(ft_frac.keys()))\n",
        "    if not layers:\n",
        "        print(\"No top-1 usage captured.\")\n",
        "        return None\n",
        "    # Determine expert count by the largest vector length\n",
        "    max_e = max([arr.shape[0] for arr in list(base_frac.values()) + list(ft_frac.values())])\n",
        "\n",
        "    def stack(frac_dict):\n",
        "        mat = np.zeros((len(layers), max_e), dtype=np.float32)\n",
        "        for i, l in enumerate(layers):\n",
        "            arr = frac_dict.get(l)\n",
        "            if arr is not None:\n",
        "                mat[i, : arr.shape[0]] = arr\n",
        "        return mat\n",
        "\n",
        "    base_mat = stack(base_frac)\n",
        "    ft_mat = stack(ft_frac)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
        "    sns.heatmap(base_mat, ax=axes[0], cmap=\"magma\", cbar=True)\n",
        "    axes[0].set_title(\"Base: Top-1 Expert Fraction per Layer\")\n",
        "    axes[0].set_xlabel(\"Expert\")\n",
        "    axes[0].set_ylabel(\"Layer (0=bottom)\")\n",
        "\n",
        "    sns.heatmap(ft_mat, ax=axes[1], cmap=\"magma\", cbar=True)\n",
        "    axes[1].set_title(\"BlockFFN-LoRA: Top-1 Expert Fraction per Layer\")\n",
        "    axes[1].set_xlabel(\"Expert\")\n",
        "\n",
        "    plt.tight_tight_layout = plt.tight_layout\n",
        "    plt.tight_tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_results(out_dir: str, base_metrics, ft_metrics, base_frac, ft_frac):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    with open(os.path.join(out_dir, \"metrics_base.json\"), \"w\") as f:\n",
        "        json.dump(base_metrics, f, indent=2)\n",
        "    with open(os.path.join(out_dir, \"metrics_ft.json\"), \"w\") as f:\n",
        "        json.dump(ft_metrics, f, indent=2)\n",
        "    # Save fraction matrices as CSVs per layer\n",
        "    def save_frac(prefix, frac):\n",
        "        for l, arr in frac.items():\n",
        "            pd.Series(arr).to_csv(os.path.join(out_dir, f\"{prefix}_layer{l}.csv\"), index_label=\"expert\", header=[\"fraction\"])\n",
        "    save_frac(\"base_top1\", base_frac)\n",
        "    save_frac(\"ft_top1\", ft_frac)\n",
        "\n",
        "    # Plots (save the figures created by the plotting functions)\n",
        "    fig1 = plots_compare_metrics(base_metrics, ft_metrics)\n",
        "    if fig1 is not None:\n",
        "        fig1.savefig(os.path.join(out_dir, \"metrics_compare.png\"), bbox_inches=\"tight\")\n",
        "        plt.close(fig1)\n",
        "\n",
        "    fig2 = plots_compare_heatmaps(base_frac, ft_frac)\n",
        "    if fig2 is not None:\n",
        "        fig2.savefig(os.path.join(out_dir, \"top1_heatmaps.png\"), bbox_inches=\"tight\")\n",
        "        plt.close(fig2)\n",
        "\n",
        "    print(f\"Saved results to: {out_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 50 gsm8k/main:test samples for profiling.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\n",
            "Profiling openai/gpt-oss-20b: 100%|██████████| 50/50 [00:01<00:00, 28.98it/s]\n",
            "Loading checkpoint shards: 100%|██████████| 9/9 [00:07<00:00,  1.20it/s]\n",
            "Profiling mcemri/gpt-oss-20b-blockffn-lora: 100%|██████████| 50/50 [00:01<00:00, 29.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No metrics to plot.\n",
            "No top-1 usage captured.\n",
            "No metrics to plot.\n",
            "No top-1 usage captured.\n",
            "Saved results to: /data/mert_cemri/LLaMA-Factory/saves/profiling_gsm8k_20251117-092331\n"
          ]
        }
      ],
      "source": [
        "samples = load_gsm8k(NUM_SAMPLES, SPLIT)\n",
        "print(f\"Loaded {len(samples)} {DATASET_NAME}/{DATASET_CONFIG}:{SPLIT} samples for profiling.\")\n",
        "\n",
        "base_metrics, base_frac = profile_model(samples, BASE_MODEL_ID)\n",
        "ft_metrics, ft_frac = profile_model(samples, FT_MODEL_ID)\n",
        "\n",
        "# Display plots inline using returned figures\n",
        "fig = plots_compare_metrics(base_metrics, ft_metrics)\n",
        "if fig is not None:\n",
        "    plt.show()\n",
        "fig = plots_compare_heatmaps(base_frac, ft_frac)\n",
        "if fig is not None:\n",
        "    plt.show()\n",
        "\n",
        "# Save\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "out_dir = f\"/data/mert_cemri/LLaMA-Factory/saves/profiling_gsm8k_{timestamp}\"\n",
        "save_results(out_dir, base_metrics, ft_metrics, base_frac, ft_frac)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "moeft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
